description: finetuning test on AMLK8s

target:
  service: amlk8s
  name: ms-shared-v100

environment:
  image: sonichi/hpo4hf:latest
  setup:
    # - pip install transformers datasets wandb prompt_toolkit==1.0.14 --user
    # - sudo apt-get -y install git
    # - pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html
    - git clone https://github.com/liususan091219/FLAML.git
    - cd FLAML
    - git checkout exp
    - cd ..
    - pip install git+https://github.com/microsoft/deepspeed
    - git clone https://github.com/huggingface/transformers
    - cd transformers
    - git checkout 1c06240e1b34777281
    - pip install -r examples/_tests_requirements.txt --user
    - pip install git+https://github.com/huggingface/transformers.git@1c06240e1b34777281

env_defaults:
  BS: 4

#code:
#  local_dir: $CONFIG_DIR/../

jobs:
- name: test_deepspeed
  sku: 16G4
  command:
  - USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 FLAML/test/hf/run_translation.py --model_name_or_path t5-3b --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps --do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 2 --overwrite_output_dir  --per_device_train_batch_size $BS --per_device_eval_batch_size $BS --predict_with_generate --sortish_sampler --val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_val_samples 500 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro --source_prefix "translate English to Romanian:" --deepspeed FLAML/test/hf/ds_configs/ds_config_1gpu.json --fp16
  submit_args: &retry_args
    max_attempts: 0