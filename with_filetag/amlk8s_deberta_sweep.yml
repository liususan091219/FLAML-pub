description: finetuning test on AMLK8s

target:
  service: amlk8s
  name: ms-shared

environment:
  image: sonichi/hpo4hf:latest
  setup:
    # - pip install transformers datasets wandb prompt_toolkit==1.0.14 --user
    # - sudo apt-get -y install git
    - git clone https://github.com/liususan091219/FLAML.git
    - cd FLAML
    - git checkout exp
    - pip install -e.[blendsearch,ray] --user
    - pip install azure-storage-blob

code:
  local_dir: $CONFIG_DIR/../key/

search:
  job_template:
    name: _{experiment_name:s}_{auto:3s}
    sku: G4-V100
    command:
    - python FLAML/test/hf/run_autohf.py --dataset_subdataset_name {dataset_subdataset_name} --pretrained_model_size {pretrained_model_size} --learning_rate {learning_rate} --weight_decay {weight_decay} --key_path "." --sample_num 1 --time_budget 100000 --root_log_path "logs_seed/" --resplit_mode rspt --yml_file amlk8s_deberta_sweep.yml
    submit_args: &retry_args
      max_attempts: 0
  type: grid
  max_trials: 18
  params:
    - name: pretrained_model_size
      spec: discrete
      values: ["microsoft/deberta-base:base"]
    - name: dataset_subdataset_name
      spec: discrete
      values: ["glue:cola", "glue:mrpc", "glue:rte"]
    - name: learning_rate
      spec: discrete
      values: [1e-5, 2e-5, 3e-5]
    - name: weight_decay
      spec: discrete
      values: [0.0, 0.1]